Linear Regression: Used for predicting a continuous target variable based on one or more input features,
    assuming a linear relationship.

Logistic Regression: Primarily used for binary classification problems, where the goal is
   to predict one of two classes.

Decision Trees: Tree-like models used for both classification 
    and regression tasks, making decisions by following a set of rules based on input features.

Random Forest: An ensemble method that combines multiple decision trees to improve prediction accuracy 
    and reduce overfitting.

Support Vector Machines (SVM): Used for classification tasks,
   SVM tries to find the optimal hyperplane that best separates data points belonging to different classes.

Naive Bayes: A probabilistic classifier
    based on Bayes' theorem, often used for text classification and spam filtering.

K-Nearest Neighbors (KNN): A simple instance-based learning algorithm used for classification
    and regression tasks, where predictions are based on the majority class of the k-nearest data points.

K-Means: An unsupervised clustering algorithm that groups similar
    data points into clusters based on their similarity.

Principal Component Analysis (PCA): A dimensionality reduction technique
    used to reduce the number of features in a dataset while preserving its variance.

Gradient Boosting Machines (e.g., XGBoost, LightGBM): Ensemble learning methods
    that build a strong predictive model by combining the predictions of multiple weak models sequentially.

Neural Networks (Multilayer Perceptrons): While deep learning has gained prominence in recent years,
    traditional neural networks are also considered part of traditional machine learning. They consist of layers of interconnected nodes and are used for various tasks, including classification and regression.